{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2548c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cbe8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 09:38:51 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream.format('socket')\\\n",
    "                        .option('host','localhost')\\\n",
    "                        .option('port',12345)\\\n",
    "                        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1977c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode('append')\\\n",
    "                        .format('console')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c372c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 09:40:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bcc84238-e4e8-41da-b922-36cfe915f940. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 09:40:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   Hi|\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|Hello everybody|\n",
      "+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|PysPark|\n",
      "+-------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|ksdjlkjflkjsfljsk...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf29678",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da757022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 10:47:07 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "df_stream = spark.readStream.format('socket')\\\n",
    ".option('host','localhost')\\\n",
    ".option('port',12345)\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed6e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f9f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a280ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_splitted = df_stream.select(split(df_stream['value'],' ').alias('splittedlines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e75dd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[splittedlines: array<string>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824daa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- splittedlines: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines_splitted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420bd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines_splitted.select(explode(lines_splitted['splittedlines']).alias('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4d1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c976cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounst = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab93c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordcounst.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d15104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new = df_stream.select(explode(split(df_stream['value'],' ')).alias('word'))\\\n",
    ".groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b35222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = words_new.writeStream.format('console').outputMode('complete')\\\n",
    ".trigger(processingTime='1 second')\\\n",
    ".option('checkpointLocation',checkpointDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39c84a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_0 = df_stream.writeStream.format('console').outputMode('append').trigger(processingTime='1 second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13368a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_1 = lines_splitted.writeStream.format('console').outputMode('append').trigger(processingTime='1 second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d807b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_2 = words.writeStream.format('console').outputMode('append').trigger(processingTime='1 second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "515ce85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_3 = wordcounst.writeStream.format('console')\\\n",
    ".outputMode('complete')\\\n",
    ".trigger(processingTime='1 second')\\\n",
    ".option('checkpointLocation',checkpointDir3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df0ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = 'chkpnt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a45f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir3 = 'chkpnt3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "201302a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:12:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:13:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 61934 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  you|    1|\n",
      "|  how|    1|\n",
      "|   Hi|    4|\n",
      "|Hello|    3|\n",
      "| you?|    1|\n",
      "|  are|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:14:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 29789 milliseconds\n",
      "22/06/14 11:14:52 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 24380 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  you|    1|\n",
      "|  how|    1|\n",
      "|   Hi|    5|\n",
      "|Hello|    3|\n",
      "| you?|    2|\n",
      "|  are|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  you|    1|\n",
      "|  how|    1|\n",
      "|   Hi|    6|\n",
      "|Hello|    3|\n",
      "| you?|    2|\n",
      "|  are|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:16:43 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 29167 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad6095e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34df31e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:19:09 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-29fc1243-e837-4b86-85c6-fa0c224b15a5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 11:19:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------+\n",
      "|splittedlines|\n",
      "+-------------+\n",
      "+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|       splittedlines|\n",
      "+--------------------+\n",
      "|[Hi, how, are, yo...|\n",
      "+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|       splittedlines|\n",
      "+--------------------+\n",
      "|[Hi, Hello, Hi, H...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1 = writer_1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ad72d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dbb9397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:20:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-feb98492-5731-4f2a-8c19-af0ab4928046. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 11:20:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:20:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1108 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|  Hi|\n",
      "| how|\n",
      "| are|\n",
      "| you|\n",
      "|   ?|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = writer_2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39102001",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2778df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:22:27 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "22/06/14 11:22:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:23:07 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 30000 milliseconds, but spent 38921 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|   Hi|    2|\n",
      "|Hello|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 12345)\n",
    "         .load())\n",
    "#words = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "checkpointDir = \"chkpnt\"\n",
    "streamingQuery = (counts.writeStream\n",
    "                  .format(\"console\")\n",
    "                  .outputMode(\"complete\")\n",
    "                  .trigger(processingTime=\"30 second\")\n",
    "                  .option(\"checkpointLocation\", 'chk')\n",
    "                  .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d772e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199d0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8e3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('text').load('MyInputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d302af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode('append')\\\n",
    ".format('console')\\\n",
    ".option('truncate',False)\\\n",
    ".option('numRows',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7876bda3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:43:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ed5f4d24-cf01-4fb1-8a9f-c50ca5e48017. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 11:43:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------------------------+\n",
      "|value                                 |\n",
      "+--------------------------------------+\n",
      "|date,delay,distance,origin,destination|\n",
      "|1011245,6,602,ABE,ATL                 |\n",
      "|1020600,-8,369,ABE,DTW                |\n",
      "|1021245,-2,602,ABE,ATL                |\n",
      "|1020605,-4,602,ABE,ATL                |\n",
      "|1031245,-4,602,ABE,ATL                |\n",
      "|1030605,0,602,ABE,ATL                 |\n",
      "|1041243,10,602,ABE,ATL                |\n",
      "|1040605,28,602,ABE,ATL                |\n",
      "|1051245,88,602,ABE,ATL                |\n",
      "+--------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------------------------+\n",
      "|value                                 |\n",
      "+--------------------------------------+\n",
      "|date,delay,distance,origin,destination|\n",
      "|1080600,0,369,ABE,DTW                 |\n",
      "|1081230,33,369,ABE,DTW                |\n",
      "|1080625,1,602,ABE,ATL                 |\n",
      "|1080607,5,569,ABE,ORD                 |\n",
      "|1081219,54,569,ABE,ORD                |\n",
      "|1091215,43,602,ABE,ATL                |\n",
      "|1090600,151,369,ABE,DTW               |\n",
      "|1091725,0,602,ABE,ATL                 |\n",
      "|1091230,-4,369,ABE,DTW                |\n",
      "+--------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|PAR1\u0015\u0000\u0015�\u0003\u0015�\u0001\u0015����\u0003\u001c",
      "\u0015(\u0015\u0000\u0015\u0006\u0015\\b\u0000\u0000�\u0001@\u0002\u0000\u0000\u0000(\u0001\\a\u0000\u0000\u00001080600                                                                                                                                                                                                                                                                                                                     |\n",
      "|\\v\\b123\u0011\\v\\f0625                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|\u0016\\f0607                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|\\v\\f1219\\t\\v\\f9121                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|!\u00009\u0019B\\b917\u00117\u00009\u0019M\u00009\u0019M\u00017\\tB\u000410\u0015B\u000410\u0015B\u000410\u0015B\u0001!                                                                                                                                                                                                                                                                                                                              |\n",
      "|�\u0001!                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|X\u0001\u0016\u0011B\u00001\u0019B<10600\\a\u0000\u0000\u00001110625\u0015\u0004\u0015�\u0001\u0015�\u0001\u0015��ʳ\u0003<\u0015 \u0015\u0004\u0000\u0000@�?\u0000\u0000\u0000\u0000!\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0005\u0000\u0000\u00006\u0000\u0000\u0000+\u0000\u0000\u0000�\u0000\u0000\u0000����\\b\u0000\u0000\u0000S\u0000\u0000\u0000����\\a\u0000\u0000\u0000����4\u0000\u0000\u0000\u0000\u0000\u0000����\u0015\u0000\u0015(\u0015,\u0015���\\v\u001c",
      "\u0015(\u0015\u0004\u0015\u0006\u0015\\b\u0000\u0000\u0014L\u0002\u0000\u0000\u0000(\u0001\u0004\\a\u00102T\u0006�����\u0000\u0000\u0015\u0004\u0015\u0018\u0015\u001c",
      "\u0015�׬�\u000e<\u0015\u0006\u0015\u0004\u0000\u0000\\f,q\u0001\u0000\u0000Z\u0002\u0000\u00009\u0002\u0000\u0000\u0015\u0000\u0015\u001c",
      "\u0015 \u0015����\u0001\u001c",
      "\u0015(\u0015\u0004\u0015\u0006\u0015\\b\u0000\u0000\u000e4\u0002\u0000\u0000\u0000(\u0001\u0002\\a�FdDF\u0000\u0015\u0004\u0015\u000e\u0015\u0012\u0015駆�                                                                                                                 |\n",
      "|<\u0015\u0002\u0015\u0004\u0000\u0000\\a\u0018\u0003\u0000\u0000\u0000ABE\u0015\u0000\u0015\u0010\u0015\u0014\u0015��                                                                                                                                                                                                                                                                                                                                              |\n",
      "|\u001c",
      "\u0015(\u0015\u0004\u0015\u0006\u0015\\b\u0000\u0000\\b\u001c",
      "\u0002\u0000\u0000\u0000(\u0001\u0000(\u0015\u0004\u0015*\u0015.\u0015𭞆\\a<\u0015\u0006\u0015\u0004\u0000\u0000\u0015P\u0003\u0000\u0000\u0000DTW\u0003\u0000\u0000\u0000ATL\u0003\u0000\u0000\u0000ORD\u0015\u0000\u0015\u001c",
      "\u0015 \u0015����\u0001\u001c",
      "\u0015(\u0015\u0004\u0015\u0006\u0015\\b\u0000\u0000\u000e4\u0002\u0000\u0000\u0000(\u0001\u0002\\a�FdDF\u0000\u0019\u0011\u0002\u0019\u0018\\a1080600\u0019\u0018\\a1111215\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\u0004����\u0019\u0018\u0004�\u0000\u0000\u0000\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\u0004q\u0001\u0000\u0000\u0019\u0018\u0004Z\u0002\u0000\u0000\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\u0003ABE\u0019\u0018\u0003ABE\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u0011\u0002\u0019\u0018\u0003ATL\u0019\u0018\u0003ORD\u0015\u0002\u0019\u0016\u0000\u0000\u0019\u001c",
      "\u0016\\b\u0015�\u0002\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\u0003\u0015Z\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\u0005\u0015N\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\u0006\u0015B\u0016\u0000\u0000\u0000\u0019\u001c",
      "\u0016�\\a\u0015N\u0016\u0000\u0000\u0000\u0015\u0002\u0019lH\\fspark_schema\u0015                                                           |\n",
      "|\u0000\u0015\\f%\u0002\u0018\u0004date%\u0000L\u001c",
      "\u0000\u0000\u0000\u0015\u0002%\u0002\u0018\u0005delay\u0000\u0015\u0002%\u0002\u0018\\bdistance\u0000\u0015\\f%\u0002\u0018\u0006origin%\u0000L\u001c",
      "\u0000\u0000\u0000\u0015\\f%\u0002\u0018\\vdestination%\u0000L\u001c",
      "\u0000\u0000\u0000\u0016(\u0019\u001c",
      "\u0019\\&\\b\u001c",
      "\u0015\\f\u00195\u0006\\b\u0000\u0019\u0018\u0004date\u0015\u0002\u0016(\u0016�\u0003\u0016�\u0002&\\b<6\u0000(\\a1111215\u0018\\a1080600\u0000\u0019\u001c",
      "\u0015\u0000\u0015\u0000\u0015\u0002\u0000\u0000\u0016�\\t\u0015\u0016\u0016�\\a\u0015:\u0000&�\u0003\u001c",
      "\u0015\u0002\u00195\u0006\u0004\\b\u0019\u0018\u0005delay\u0015\u0002\u0016(\u0016�\u0002\u0016�\u0002&�\u0003&�\u0002\u001c",
      "\u0018\u0004�\u0000\u0000\u0000\u0018\u0004����\u0016\u0000(\u0004�\u0000\u0000\u0000\u0018\u0004����\u0000\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0000\u0015\u0000\u0015\u0004\u0015\u0002\u0000\u0000\u0016�\\t\u0015\u0016\u0016�\\b\u0015.\u0000&�\u0005\u001c",
      "\u0015\u0002\u00195\u0006\u0004\\b\u0019\u0018\\bdistance\u0015\u0002\u0016(\u0016�\u0001\u0016�\u0001&�\u0005&�\u0004\u001c",
      "\u0018\u0004Z\u0002\u0000\u0000\u0018\u0004q\u0001\u0000\u0000\u0016\u0000(\u0004Z\u0002\u0000\u0000\u0018\u0004q\u0001\u0000\u0000\u0000\u0019,\u0015\u0004\u0015\u0004\u0015\u0002\u0000\u0015\u0000\u0015\u0004\u0015\u0002\u0000\u0000\u0016�|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------------------------+\n",
      "|value                                 |\n",
      "+--------------------------------------+\n",
      "|date,delay,distance,origin,destination|\n",
      "|1011245,6,602,ABE,ATL                 |\n",
      "|1020600,-8,369,ABE,DTW                |\n",
      "|1021245,-2,602,ABE,ATL                |\n",
      "|1020605,-4,602,ABE,ATL                |\n",
      "|1031245,-4,602,ABE,ATL                |\n",
      "|1030605,0,602,ABE,ATL                 |\n",
      "|1041243,10,602,ABE,ATL                |\n",
      "|1040605,28,602,ABE,ATL                |\n",
      "|1051245,88,602,ABE,ATL                |\n",
      "+--------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ca4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c8f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField,\n",
    "                               StringType, IntegerType)\n",
    "\n",
    "recordSchema = StructType([StructField('date', StringType(), True),\n",
    "                           StructField('delay', IntegerType(), True),\n",
    "                           StructField('distance', IntegerType(), True),\n",
    "                           StructField('origin', StringType(), True),\n",
    "                           StructField('destination', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a76bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordSchema2 = StructType([StructField('date', StringType(), True),\n",
    "                           StructField('delay', IntegerType(), True),\n",
    "                           StructField('distance', IntegerType(), True),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('csv')\\\n",
    ".schema(recordSchema)\\\n",
    ".load('MyInputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54d38427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('parquet')\\\n",
    ".schema(recordSchema)\\\n",
    ".load('MyInputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306d4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('parquet')\\\n",
    ".schema(recordSchema2)\\\n",
    ".load('MyInputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e8890f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode('append')\\\n",
    ".format('console')\\\n",
    ".option('truncate',False)\\\n",
    ".option('numRows',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "753e742b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 11:56:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c67d5288-9974-408e-bf97-46772a8d52df. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 11:56:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+-----+--------+\n",
      "|date   |delay|distance|\n",
      "+-------+-----+--------+\n",
      "|1080600|0    |369     |\n",
      "|1081230|33   |369     |\n",
      "|1080625|1    |602     |\n",
      "|1080607|5    |569     |\n",
      "|1081219|54   |569     |\n",
      "|1091215|43   |602     |\n",
      "|1090600|151  |369     |\n",
      "|1091725|0    |602     |\n",
      "|1091230|-4   |369     |\n",
      "|1090625|8    |602     |\n",
      "+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b583da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eedd3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 12:56:40 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream.format('socket')\\\n",
    ".option('host','localhost')\\\n",
    ".option('port',12345)\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c1ffb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode('append')\\\n",
    ".format('text')\\\n",
    ".option('path','OutStream/')\\\n",
    ".option('checkpointLocation','chkpnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaece175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 13:00:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1a2076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               word|count|\n",
      "+-------------------+-----+\n",
      "|      [Hello, Hi, ]|    1|\n",
      "|[Hi, how, are, you]|    1|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mytable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef395130",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb97ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('csv')\\\n",
    ".schema(recordSchema)\\\n",
    ".load('MyInputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a698f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"memory\")  \\\n",
    "    .queryName('myTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17eba685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 13:32:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9ad0dad3-7415-4ab0-897f-27a7ac67b552. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 13:32:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87b235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|   date| null|    null|origin|destination|\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "+-------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM myTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c5c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21c9a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupBy('origin').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c53eb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df2.writeStream.outputMode(\"complete\") \\\n",
    "    .format(\"memory\")  \\\n",
    "    .queryName('myTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d520956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 13:34:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ccba14f3-a0ad-40d5-94fa-e89d94cabe41. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/06/14 13:34:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 12:>                                                       (2 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "query= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29909097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                       (3 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|origin|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM myTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19695c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|origin|count|\n",
      "+------+-----+\n",
      "|   ABE|   19|\n",
      "|origin|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM myTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93b7ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|origin|count|\n",
      "+------+-----+\n",
      "|   ABE|   19|\n",
      "|origin|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM myTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b6f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
